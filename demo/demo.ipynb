{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaslim/miniconda3/envs/cs6120-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import IPython\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'head': subject.strip(),\n",
    "                    'type': relation.strip(),\n",
    "                    'tail': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'head': subject.strip(),\n",
    "            'type': relation.strip(),\n",
    "            'tail': object_.strip()\n",
    "        })\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.entities = set() # { entity_title: {...} }\n",
    "        self.relations = [] # [ head: entity_title, type: ..., tail: entity_title,\n",
    "          # meta: { article_url: { spans: [...] } } ]\n",
    "        self.sources = {} # { article_url: {...} }\n",
    "\n",
    "    def merge_with_kb(self, kb2):\n",
    "        for r in kb2.relations:\n",
    "            article_url = list(r[\"meta\"].keys())[0]\n",
    "            source_data = kb2.sources[article_url]\n",
    "            self.add_relation(r, source_data[\"article_title\"],\n",
    "                              source_data[\"article_publish_date\"])\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    def merge_relations(self, r2):\n",
    "        r1 = [r for r in self.relations\n",
    "              if self.are_relations_equal(r2, r)][0]\n",
    "\n",
    "        # if different article\n",
    "        article_url = list(r2[\"meta\"].keys())[0]\n",
    "        if article_url not in r1[\"meta\"]:\n",
    "            r1[\"meta\"][article_url] = r2[\"meta\"][article_url]\n",
    "\n",
    "        # if existing article\n",
    "        else:\n",
    "            spans_to_add = [span for span in r2[\"meta\"][article_url][\"spans\"]\n",
    "                            if span not in r1[\"meta\"][article_url][\"spans\"]]\n",
    "            r1[\"meta\"][article_url][\"spans\"] += spans_to_add\n",
    "\n",
    "    def add_entity(self, e):\n",
    "        self.entities.add(e)\n",
    "\n",
    "    def add_relation(self, r, article_title, article_publish_date):\n",
    "        entities = [r[\"head\"], r[\"tail\"]]\n",
    "\n",
    "        for e in entities:\n",
    "            self.add_entity(e)\n",
    "\n",
    "        # rename relation entities with their wikipedia titles\n",
    "        r[\"head\"] = entities[0]\n",
    "        r[\"tail\"] = entities[1]\n",
    "\n",
    "        # add source if not in kb\n",
    "        article_url = list(r[\"meta\"].keys())[0]\n",
    "        if article_url not in self.sources:\n",
    "            self.sources[article_url] = {\n",
    "                \"article_title\": article_title,\n",
    "                \"article_publish_date\": article_publish_date\n",
    "            }\n",
    "\n",
    "        # manage new relation\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Entities:\")\n",
    "        for e in self.entities.items():\n",
    "            print(f\"  {e}\")\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "        print(\"Sources:\")\n",
    "        for s in self.sources.items():\n",
    "            print(f\"  {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_kb(text, article_url, span_length=128, article_title=None,\n",
    "                    article_publish_date=None, verbose=False):\n",
    "    # tokenize whole text\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "    # compute span boundaries\n",
    "    num_tokens = len(inputs[\"input_ids\"][0])\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_tokens} tokens\")\n",
    "    num_spans = math.ceil(num_tokens / span_length)\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_spans} spans\")\n",
    "    overlap = math.ceil((num_spans * span_length - num_tokens) / \n",
    "                        max(num_spans - 1, 1))\n",
    "    spans_boundaries = []\n",
    "    start = 0\n",
    "    for i in range(num_spans):\n",
    "        spans_boundaries.append([start + span_length * i,\n",
    "                                 start + span_length * (i + 1)])\n",
    "        start -= overlap\n",
    "    if verbose:\n",
    "        print(f\"Span boundaries are {spans_boundaries}\")\n",
    "\n",
    "    # transform input with spans\n",
    "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
    "                  for boundary in spans_boundaries]\n",
    "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
    "                    for boundary in spans_boundaries]\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.stack(tensor_ids),\n",
    "        \"attention_mask\": torch.stack(tensor_masks)\n",
    "    }\n",
    "\n",
    "    # generate relations\n",
    "    num_return_sequences = 20\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 20,\n",
    "        \"num_return_sequences\": num_return_sequences\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    # decode relations\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
    "                                           skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    kb = KB()\n",
    "    i = 0\n",
    "    for sentence_pred in decoded_preds:\n",
    "        current_span_index = i // num_return_sequences\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for relation in relations:\n",
    "            relation[\"meta\"] = {\n",
    "                article_url: {\n",
    "                    \"spans\": [spans_boundaries[current_span_index]]\n",
    "                }\n",
    "            }\n",
    "            kb.add_relation(relation, article_title, article_publish_date)\n",
    "        i += 1\n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_html(kb, filename=\"network.html\"):\n",
    "    # create network\n",
    "    net = Network(directed=True, width=\"700px\", height=\"700px\", bgcolor=\"#eeeeee\")\n",
    "\n",
    "    # nodes\n",
    "    color_entity = \"#00FF00\"\n",
    "    for e in kb.entities:\n",
    "        net.add_node(e, shape=\"circle\", color=color_entity)\n",
    "\n",
    "    # edges\n",
    "    for r in kb.relations:\n",
    "        net.add_edge(r[\"head\"], r[\"tail\"],\n",
    "                    title=r[\"type\"], label=r[\"type\"])\n",
    "        \n",
    "    # save network\n",
    "    net.repulsion(\n",
    "        node_distance=200,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    net.show(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 14 tokens\n",
      "Input has 1 spans\n",
      "Span boundaries are [[0, 128]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "text = \"Donald Trump bsdflearded about the new tax legislation\"\n",
    "kb = from_text_to_kb(text, \"\", verbose=True)\n",
    "filename = \"kg.html\"\n",
    "save_network_html(kb, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 10 tokens\n",
      "Input has 1 spans\n",
      "Span boundaries are [[0, 128]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'head': 'Purex',\n",
       "  'type': 'product or material produced',\n",
       "  'tail': 'lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'product or material produced',\n",
       "  'tail': 'lipolizes EColi',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'subject has role',\n",
       "  'tail': 'lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'subject has role',\n",
       "  'tail': 'lipolizes EColi',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'product or material produced',\n",
       "  'tail': 'lipolize',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'subject has role',\n",
       "  'tail': 'lipolize',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'EColi',\n",
       "  'type': 'drug used for treatment',\n",
       "  'tail': 'Purex lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'instance of',\n",
       "  'tail': 'lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'EColi',\n",
       "  'type': 'drug used for treatment',\n",
       "  'tail': 'Purex',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex',\n",
       "  'type': 'use',\n",
       "  'tail': 'lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex lipolize EColi',\n",
       "  'type': 'instance of',\n",
       "  'tail': 'lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex lipolizes EColi',\n",
       "  'type': 'instance of',\n",
       "  'tail': 'lipolizes',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}},\n",
       " {'head': 'Purex lipolizes',\n",
       "  'type': 'product or material produced',\n",
       "  'tail': 'EColi',\n",
       "  'meta': {'': {'spans': [[0, 128]]}}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Purex lipolizes EColi\"\n",
    "kb = from_text_to_kb(text, \"\", verbose=True)\n",
    "kb.relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aijournalism",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
